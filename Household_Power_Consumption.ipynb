{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBq13SamwUEt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prediction of Household Electricity Consumption 1\n",
        "\n",
        "# Step 1 (clean, multivaraite, fillna the dataset)\n",
        "\n",
        "#Include LIbraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "#Read Txt file\n",
        "df = pd.read_csv(\"/content/Household Power Consumption.csv\", sep=',', na_values='?', low_memory=False)\n",
        "print(df.head())\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Features combine date-time in one column (datetime)\n",
        "df['Datetime'] = pd.to_datetime(df['Date']+' '+df['Time'], format='mixed', dayfirst=True)\n",
        "df.drop(['Date','Time'], axis=1, inplace=True) # Now we drop the date and time column\n",
        "df.set_index('Datetime', inplace=True) # Now we set the datetime column as index\n",
        "df.head()\n",
        "\n",
        "#convert features into numeric\n",
        "df = df.astype(float)\n",
        "\n",
        "#Target & Features (Multivariate Setup)\n",
        "features = [\n",
        "    'Global_active_power',\n",
        "    'Global_reactive_power',\n",
        "    'Voltage',\n",
        "    'Global_intensity',\n",
        "    'Sub_metering_1',\n",
        "    'Sub_metering_2',\n",
        "    'Sub_metering_3'\n",
        "]\n",
        "\n",
        "df = df[features]\n",
        "\n",
        "df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "#Resample\n",
        "df_hourly = df.resample('H').mean()\n",
        "print(df_hourly.shape)\n",
        "df_hourly.head()\n",
        "\n",
        "#Visulaisation\n",
        "df_hourly['Global_active_power'].plot(figsize=(16,6))\n",
        "plt.title('Hourly_Global_active_power')\n",
        "plt.show()\n",
        "\n",
        "result = adfuller(df_hourly['Global_active_power'])\n",
        "\n",
        "print(\"ADF Statistic:\", result[0])\n",
        "print(\"p-value:\", result[1])\n",
        "\n",
        "df_hourly.to_csv(\"household_power_hourly.csv\")\n",
        "\n",
        "#Step 2 BASELINE FORECASTING MODEL (ARIMA)\n",
        "\n",
        "series = df_hourly['Global_active_power']\n",
        "train_size = int(len(series) * 0.8)\n",
        "train = series[:train_size]\n",
        "test = series[train_size:]\n",
        "\n",
        "#train ARIMA model\n",
        "model = ARIMA(train, order=(2,1,2))\n",
        "model_fit = model.fit()\n",
        "\n",
        "predictions = model_fit.forecast(steps=len(test))\n",
        "\n",
        "arima_rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "arima_mae = mean_absolute_error(test, predictions)\n",
        "\n",
        "print(\"ARIMA RMSE:\", arima_rmse)\n",
        "print(\"ARIMA MAE:\", arima_mae)\n",
        "\n",
        "#Visuvalisation\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test.index, test, label=\"Actual\")\n",
        "plt.plot(test.index, predictions, label=\"ARIMA Forecast\")\n",
        "plt.legend()\n",
        "plt.title(\"ARIMA Baseline Forecast\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#**Step 3** LSTM\n",
        "\n",
        "#Time-based split\n",
        "data = df_hourly.values\n",
        "\n",
        "train_size = int(len(data) * 0.7)\n",
        "val_size = int(len(data) * 0.85)\n",
        "\n",
        "train = data[:train_size]\n",
        "val = data[train_size:val_size]\n",
        "test = data[val_size:]\n",
        "\n",
        "#Scaling\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train)\n",
        "val_scaled = scaler.transform(val)\n",
        "test_scaled = scaler.transform(test)\n",
        "\n",
        "#Sliding-window function\n",
        "def create_sequences(data, lookback):\n",
        "    x, y = [], []\n",
        "    for i in range(len(data) - lookback):\n",
        "        x.append(data[i:i+lookback, 1:])   # input features\n",
        "        y.append(data[i+lookback, 0])      # target (Global_active_power)\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "LOOKBACK = 24\n",
        "\n",
        "x_train, y_train = create_sequences(train_scaled, LOOKBACK)\n",
        "x_val, y_val = create_sequences(val_scaled, LOOKBACK)\n",
        "x_test, y_test = create_sequences(test_scaled, LOOKBACK)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "#**step 4 LSTM MODEL DESIGN & TRAINING**\n",
        "\n",
        "#Minimal & Correct LSTM Architecture\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "timesteps = x_train.shape[1]\n",
        "features = x_train.shape[2]\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(timesteps, features)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "#Training Strategy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "#Train the LSTM\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Visualize Training\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title(\"LSTM Training Curve\")\n",
        "plt.show()\n",
        "\n",
        "#**Step 5 HYPERPARAMETER OPTIMIZATION (LSTM)**\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    units = trial.suggest_int(\"units\", 32, 128)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    model = Sequential([\n",
        "        LSTM(units, input_shape=(x_train.shape[1], x_train.shape[2])),\n",
        "        Dropout(dropout),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss='mse'\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=30,\n",
        "        batch_size=batch_size,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "#Run the Optimization Study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=25)\n",
        "\n",
        "#Best Hyperparameters Found\n",
        "print(\"Best Parameters:\")\n",
        "print(study.best_params)\n",
        "\n",
        "print(\"Best Validation Loss:\")\n",
        "print(study.best_value)\n",
        "\n",
        "#Train FINAL LSTM using best parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "final_model = Sequential([\n",
        "    LSTM(best_params['units'],\n",
        "         input_shape=(x_train.shape[1], x_train.shape[2])),\n",
        "    Dropout(best_params['dropout']),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "final_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(best_params['lr']),\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "final_model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#**Step 6 FINAL MODEL EVALUATION & COMPARISON\n",
        "\n",
        "#(ARIMA vs Optimized LSTM)\n",
        "\n",
        "lstm_pred = final_model.predict(x_test)\n",
        "\n",
        "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_pred))\n",
        "lstm_mae = mean_absolute_error(y_test, lstm_pred)\n",
        "\n",
        "print(\"LSTM RMSE:\", lstm_rmse)\n",
        "print(\"LSTM MAE:\", lstm_mae)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"ARIMA\", \"Optimized LSTM\"],\n",
        "    \"RMSE\": [arima_rmse, lstm_rmse],\n",
        "    \"MAE\": [arima_mae, lstm_mae]\n",
        "})\n",
        "\n",
        "results\n",
        "\n",
        "#Visual Comparison\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(y_test[:200], label=\"Actual\")\n",
        "plt.plot(lstm_pred[:200], label=\"LSTM Prediction\")\n",
        "plt.legend()\n",
        "plt.title(\"LSTM vs Actual (Test Set)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#** Step 7 MODEL INTERPRETABILITY**\n",
        "\n",
        "#Selecting samples to explain\n",
        "x_explain_reshaped = x_test[:3].reshape(x_test[:3].shape[0], -1)\n",
        "background_reshaped = x_train[:50].reshape(x_train[:50].shape[0], -1)\n",
        "\n",
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "\n",
        "def model_predict(X):\n",
        "    X= X.reshape((-1, x_train.shape[1], x_train.shape[2]))\n",
        "    return final_model.predict(X)\n",
        "\n",
        "explainer = shap.KernelExplainer(\n",
        "    model_predict,\n",
        "    background_reshaped\n",
        ")\n",
        "\n",
        "#Compute SHAP values\n",
        "shap_values = np.squeeze(explainer.shap_values(x_explain_reshaped))\n",
        "shap_agg = np.mean(np.abs(shap_values), axis=0)\n",
        "\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    x_explain_reshaped,\n",
        "    feature_names=[f\"t-{i}\" for i in range(x_explain_reshaped.shape[1])]\n",
        ")\n",
        "\n",
        "\n",
        "#Create directories if they don't exist\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "#Save\n",
        "final_model.save(\"models/lstm_best.h5\")\n",
        "results.to_csv(\"results/metrics_summary.csv\", index=False)\n",
        "optuna_results = study.trials_dataframe()\n",
        "optuna_results.to_csv(\"results/optuna_trials.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}